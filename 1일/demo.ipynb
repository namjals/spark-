{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://10.10.10.100:7077\") \\\n",
    "    .appName(\"Example\") \\\n",
    "    .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실행 계획"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(sum(id)=2500000000000)]\n",
      "== Physical Plan ==\n",
      "*(7) HashAggregate(keys=[], functions=[sum(id#6L)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(6) HashAggregate(keys=[], functions=[partial_sum(id#6L)])\n",
      "      +- *(6) Project [id#6L]\n",
      "         +- *(6) SortMergeJoin [id#6L], [id#2L], Inner\n",
      "            :- *(3) Sort [id#6L ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(id#6L, 200)\n",
      "            :     +- *(2) Project [(id#0L * 5) AS id#6L]\n",
      "            :        +- Exchange RoundRobinPartitioning(5)\n",
      "            :           +- *(1) Range (2, 10000000, step=2, splits=3)\n",
      "            +- *(5) Sort [id#2L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(id#2L, 200)\n",
      "                  +- Exchange RoundRobinPartitioning(6)\n",
      "                     +- *(4) Range (2, 10000000, step=4, splits=3)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.range(2, 10000000, 2) # 2,4,6,... 2개 파티션\n",
    "df2 = spark.range(2, 10000000, 4) # 2,6,10,... 3개 파티션\n",
    "step1 = df1.repartition(5) # 셔플링\n",
    "step12 = df2.repartition(6) # 셔플링\n",
    "step2 = step1.selectExpr(\"id * 5 as id\") # 10, 20, 30,...\n",
    "step3 = step2.join(step12, [\"id\"]) # 셔플링\n",
    "step4 = step3.selectExpr(\"sum(id)\") \n",
    "print step4.collect() # Action, 결과 전송을 위한 셔플링\n",
    "print step4.explain() # 실행계획 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DataFrame -> RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[4] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10).rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 파이썬 객체로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[5] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataSource로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/home/demo/test/Spark-The-Definitive-Guide/README.md MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/demo/test/Spark-The-Definitive-Guide/README.md\"\n",
    "spark.sparkContext.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트랜스포메이션\n",
    "지연 처리 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['Simple', 'Spark', 'Data', 'The', 'Made', 'Definitive', 'Big', 'Processing', ':', 'Guide']\n"
     ]
    }
   ],
   "source": [
    "print words.distinct().count()\n",
    "print words.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark', 'Simple']\n",
      "['Spark', 'Simple']\n"
     ]
    }
   ],
   "source": [
    "def startsWithS(individual):\n",
    "    return individual.startswith(\"S\")\n",
    "\n",
    "print words.filter(lambda word: startsWithS(word)).collect()\n",
    "print words.filter(lambda word: word.startswith(\"S\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Spark', 'S', True), ('Simple', 'S', True)]\n"
     ]
    }
   ],
   "source": [
    "words2 = words.map(lambda word: (word, word[0], word.startswith(\"S\"))) # (word, word[0], True or False)\n",
    "print words2.filter(lambda record: record[2]).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'p', 'a', 'r', 'k', 'T', 'h', 'e', 'D', 'e', 'f', 'i', 'n', 'i', 't']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어를 문자 집합으로 변환\n",
    "words.flatMap(lambda word: word).take(15) # ['Spark', 'The', ...] -> ['S', 'p', 'a', 'r', 'k', 'T', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Definitive', 'Processing']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sortBy(lambda word: len(word) * -1).take(2) # -10, -9, -8 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. randomSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['Spark', 'The', 'Definitive', 'Guide', ':', 'Big', 'Data', 'Processing', 'Made', 'Simple']\n"
     ]
    }
   ],
   "source": [
    "randomSplit = words.randomSplit([0.1, 0.9])\n",
    "print randomSplit[0].collect()\n",
    "print randomSplit[1].collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 액션\n",
    "즉시 실행 방식, 데이터를 드라이버로 모으거나 외부 데이터 소스로 보냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "Processing\n"
     ]
    }
   ],
   "source": [
    "print spark.sparkContext.parallelize(range(1, 21)).reduce(lambda x, y: x + y) # 210\n",
    "\n",
    "def wordLengthReducer(leftWord, rightWord):\n",
    "    if len(leftWord) > len(rightWord):\n",
    "        return leftWord\n",
    "    else:\n",
    "        return rightWord\n",
    "\n",
    "print words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spark'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print spark.sparkContext.parallelize(range(1, 21)).max()\n",
    "print spark.sparkContext.parallelize(range(1, 21)).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. take\n",
    "먼저 하나의 파티션을 스캔, 그 다음 해당 파티션의 결과 수를 이용해 파라미터로 지정된 값을 만족하는데 필요한 추가 파티션 수를 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark']\n",
      "[':']\n"
     ]
    }
   ],
   "source": [
    "print words.take(1)\n",
    "print words.takeOrdered(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
