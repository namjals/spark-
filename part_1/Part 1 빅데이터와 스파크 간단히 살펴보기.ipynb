{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목차\n",
    "\n",
    "Part 1 빅데이터와 스파크 간단히 살펴보기\n",
    "\n",
    "Chapter 1 아파치 스파크란\n",
    "- 1.1. 아파치 스파크의 철학\n",
    "- 1.2. 스파크의 등장 배경\n",
    "- 1.3. 스파크의 역사\n",
    "- 1.4. 스파크의 현재와 미래\n",
    "- 1.5. 스파크 실행하기\n",
    "\n",
    "Chapter 2 스파크 간단히 살펴보기\n",
    "- 2.1. 스파크의 기본 아키텍처\n",
    "- 2.2. 스파크의 다양한 언어 API\n",
    "- 2.3. 스파크 API\n",
    "- 2.4. 스파크 시작하기\n",
    "- 2.5. SparkSession\n",
    "- 2.6. DataFrame\n",
    "- 2.7. 트랜스포메이션\n",
    "- 2.8. 액션\n",
    "- 2.9. 스파크 UI\n",
    "- 2.10. 종합 예제\n",
    "\n",
    "Chapter 3 스파크 기능 둘러보기\n",
    "- 3.1. 운영용 애플리케이션 실행하기\n",
    "- 3.2. Dataset: 타입 안정성을 제공하는 구조적 API\n",
    "- 3.3. 구조적 스트리밍\n",
    "- 3.4. 머신러닝과 고급 분석\n",
    "- 3.5. 저수준 API\n",
    "- 3.6. SparkR\n",
    "- 3.7. 스파크의 에코시스템과 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 빅데이터와 스파크 간단히 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 아파치 스파크란"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 통합 컴퓨팅 엔진\n",
    "- 클러스터 환경에서 데이터를 병렬 처리하는 라이브러리 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 아파치 스파크의 철학"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 통합\n",
    "    - 데이터 읽기, 쓰기, SQL 처리, 머신러닝, 스트림 처리 같은 단계를 병합하고, 같은 연산 엔진, 일관성 있는 API로 수행\n",
    "- 컴퓨팅 엔진\n",
    "    - 컴퓨팅 엔진에 집중하며 여러 저장소를 지원\n",
    "    - 하둡의 경우 맵리듀스, 하둡 파일시스템이 밀접하게 연관되어 있음\n",
    "- 라이브러리\n",
    "    - 표준 라이브러리와 함께 다양한 서드파티 라이브러리 지원\n",
    "    - 스파크 SQL, MLlib, 구조적 스트리밍, GraphX, 다양한 저장소 Connector(JDBC 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. 스파크의 등장 배경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단일 프로세서의 성능 향상의 한계가 있어, 병렬 처리가 필요해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. 스파크의 역사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 클러스터 컴퓨팅의 잠재력, 맵리듀스의 단점 보완을 위해 AMPLab이 하둡 사용자들과 함께 Spark을 만들고, Apache 제단에 기부\n",
    "- 프로젝트를 성장시키기 위해 데이터 브릭스 설립\n",
    "\n",
    "|버전|개발 내역|\n",
    "|------|---|\n",
    "|초기|함수형 연산|\n",
    "|1.0|스파크 SQL|\n",
    "|이후|DataFrame, 머신러닝, 구조적 스트리밍|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. 스파크의 현재와 미래"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 구조적 스트리밍 : 우버, 넷플릭스, NASA, CERN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. 스파크 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스파크는 스칼라로 구현되어, 자바 가상 머신 기반으로 동작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1. 로컬 환경에 스파크 내려받기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wiki.musinsa.com/pages/viewpage.action?pageId=43173002#Jupyternotebook%ED%99%98%EA%B2%BD-1.7.Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2. 스파크 대화형 콘솔 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. python spark\n",
    "> ./$SPARK_HOME/bin/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. scala spark\n",
    "> ./$SPARK_HOME/bin/spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. spark sql\n",
    "> ./$SPARK_HOME/bin/spark-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.3. 클라우드에서 스파크 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://databricks.com/try-databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.4. 이 책에서 사용하는 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Flight Data\n",
    "- Retail Data\n",
    "- Bike Data\n",
    "- Sensor Data\n",
    "\n",
    "> git clone https://github.com/FVBros/Spark-The-Definitive-Guide.git\n",
    ">\n",
    "> cd Spark-The-Definitive-Guide/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 스파크 간단히 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 스파크의 기본 아키텍처"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 클러스터 매니저 : 사용자가 제출한 잡에 대한 자원 할당\n",
    "- 스파크 : 할당받은 자원으로 데이터 처리 작업을 관리하고 조율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. 스파크 애플리케이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](img/1.jpeg)\n",
    "- 드라이버 프로세스와 다수의 익스큐터 프로세스로 구성\n",
    "- 드라이버\n",
    "    - 클러스터 노드 중 하나에서 실행\n",
    "    - main() 함수를 실행\n",
    "    - 스파크 앱의 정보 유지 관리\n",
    "    - 사용자 프로그램이나 입력에 대한 응답\n",
    "        - 클라이언트 모드로 서버에 띄울수 있음\n",
    "    - 전반적인 익스큐터 프로세스의 작업과 관련된 분석, 배포, 스케줄링 역할 수행\n",
    "        - jar 파일이나 실행 코드 배포\n",
    "    - 앱의 수명주기 동안 관련 정보를 유지\n",
    "- 익스큐터\n",
    "    - 드라이버 프로세스가 할당한 작업(코드) 수행\n",
    "    - 진행 상황을 드라이버 노드에 보고\n",
    "- 클러스터 매니저\n",
    "    - 물리적 머신 관리\n",
    "    - 스파크 앱에 자원을 할당\n",
    "    - 종류 : 스탠드얼론, YARN, 메소스\n",
    "    - 하나의 클러스터에 여러 개의 스파크 애플리케이션 실행 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 스파크의 다양한 언어 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 스파크는 모든 언어에 맞는 몇몇 '핵심 개념'을 제공\n",
    "- 이러한 핵심 개념은 클러스터 머신에서 실행되는 스파크 코드로 변환하며, 구조적 API로 작성된 코드는 언어에 상관없이 유사한 성능을 발휘\n",
    "    - API를 사용하면 내장된 최적화된 scala 코드로 변환(매핑)한다는 의미이며,그렇기 때문에 성능차이가 거의 없음\n",
    "    - 다만 udf의 경우, 변환이 안되고 각 python 프로세스를 executor에 띄우기 때문에 느린걸로 보임\n",
    "        - https://github.com/NAMYUNWOO/scalaUDFs_to_pyspark\n",
    "- 지원 언어\n",
    "    - 스칼라 : 스파크는 스칼라로 개발됨, 스파크의 기본 언어\n",
    "    - SQL : ANSI SQL:2003 표준의 일부 지원\n",
    "    - 자바, 파이썬, R\n",
    "- 파이썬이나 R로 작성한 API 코드를 익스큐터 JVM에서 실행할 수 있는 코드로 변환\n",
    "![nn](img/4.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. 스파크 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 저수준 비구조적 API(RDD), 고수준 구조적 API(DataFrame) 지원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. 스파크 시작하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대화형 모드는 SparkSession을 자동 생성\n",
    "- 애플리케이션 코드의 경우, 코드에서 SparkSession 생성 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사용자가 정의한 처리 명령을 클러스터에서 실행\n",
    "- 하나의 SparkSession은 하나의 스파크 애플리케이션에 대응\n",
    "- 대화형 콘솔의 경우 spark 변수로 사용 가능\n",
    "    - 애플리케이션 코드에서도 보통 SparkSession을 spark이라는 변수에 담아 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.200.168:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark.jupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x115593290>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 클러스터 모드에서는 숫자 범위의 각 부분이 서로 다른 익스큐터에 할당\n",
    "myRange = spark.range(1000).toDF(\"number\")\n",
    "myRange.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가장 대표적인 구조적 API\n",
    "- 테이블 데이터를 로우와 컬럼으로 표현\n",
    "- 컬럼과 컬럼 타입을 정의한 목록을 스키마\n",
    "- Spark의 DataFrame, row 단위로 여러 컴퓨터에 분산\n",
    "    - 스프레드시트, 파이썬과 R의 DataFrame은 단일 컴퓨터\n",
    "- pandas와 pyspark 간 상호 변환이 쉬움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "|  a|    b|     c|\n",
      "+---+-----+------+\n",
      "|  1| 10.0| apple|\n",
      "|  2|  3.5|banana|\n",
      "|  3|7.315|tomato|\n",
      "+---+-----+------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.000</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.500</td>\n",
       "      <td>banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7.315</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a       b       c\n",
       "0  1  10.000   apple\n",
       "1  2   3.500  banana\n",
       "2  3   7.315  tomato"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame({ \n",
    "    'a': [1, 2, 3],\n",
    "    'b': [10.0, 3.5, 7.315],\n",
    "    'c': ['apple', 'banana', 'tomato']\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(pdf) \n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|Col|Col2|\n",
      "+---+----+\n",
      "|  A|   1|\n",
      "|  B|   2|\n",
      "+---+----+\n",
      "\n",
      "root\n",
      " |-- Col: string (nullable = true)\n",
      " |-- Col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 컬럼명만 지정하면 스키마를 추론함\n",
    "data = [(\"A\", 1), (\"B\", 2)]\n",
    "schema = [\"Col\", \"Col2\"] \n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|Col|Col2|\n",
      "+---+----+\n",
      "|  A|   1|\n",
      "|  B|   2|\n",
      "+---+----+\n",
      "\n",
      "root\n",
      " |-- Col: string (nullable = true)\n",
      " |-- Col2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "data = [(\"A\", 1), (\"B\", 2)]\n",
    "schema = StructType([\n",
    "    StructField(\"Col\",StringType()),\n",
    "    StructField(\"Col2\",IntegerType()),\n",
    "])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.1. 파티션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라 불리는 청크 단위로 데이터를 분할\n",
    "- 클러스터의 물리적 머신에 존재하는 로우의 집합\n",
    "- 만약 파티션이 1개라면 스파크에 수천 개의 익스큐터가 있더라도 병렬성은 1이 됨\n",
    "- 수백개의 파티션이 있더라도 익스큐터가 하나밖에 없다면 병렬성은 1이 됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. 트랜스포메이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 액션전에 실행계획만 있는 상태\n",
    "- 스파크의 데이터구조는 불변성(immutable)을 가짐\n",
    "    - 장애복구를 위함\n",
    "- 변경 방법을 트랜스포메이션으로 스파크에 알려야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [id#1601L AS number#1603L]\n",
      "+- *(1) Filter ((id#1601L % 2) = 0)\n",
      "   +- *(1) Range (0, 1000, step=1, splits=12)\n"
     ]
    }
   ],
   "source": [
    "myRange = spark.range(1000).toDF(\"number\")\n",
    "myRange.where(\"number % 2 = 0\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 액션을 수행하기 전까지 위 코드를 실행해도 결과는 없음\n",
    "- 트랜스포매이션은 좁은 의존성, 넒은 의존성으로 나뉨\n",
    "- 좁은 의존성\n",
    "    - 각 입력 파티션이 하나의 출력 파티션에만 영향을 미침\n",
    "    - 파이프라이닝을 자동으로 수행\n",
    "        - 모든 작업이 메모리에서 일어남\n",
    "        - 결과를 중간저장하지 않고 바로 처리, map -> filter -> map\n",
    "![nn](img/6.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 넓은 의존성\n",
    "    - 하나의 입력 파티션이 여러 출력 파티션에 영향을 미침\n",
    "    - 셔플이 일어남 \n",
    "        - 셔플 : 스파크가 클러스터에서 파티션을 교환\n",
    "        - 셔플의 결과를 디스크에 저장\n",
    "            - I/O(네트워크 등)이 많이 발생하기 때문에 재 계산하지 않기 위해 저장하는게 아닐까 싶음\n",
    "            - https://stackoverflow.com/questions/34580662/what-does-stage-skipped-mean-in-apache-spark-web-ui\n",
    "![nn](img/5.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[number#1603L], functions=[sum(number#1603L)])\n",
      "+- Exchange hashpartitioning(number#1603L, 200)\n",
      "   +- *(1) HashAggregate(keys=[number#1603L], functions=[partial_sum(number#1603L)])\n",
      "      +- *(1) Project [id#1601L AS number#1603L]\n",
      "         +- *(1) Range (0, 1000, step=1, splits=12)\n"
     ]
    }
   ],
   "source": [
    "# 동일한 키를 가진 데이터를 하나의 파티션으로 모음\n",
    "# 유저별 구매원가의 합을 구한다면, 특정 유저의 모든 row는 특정 파티션에 모여야함\n",
    "# groupbykey, reducebykey, sort, join, repartition\n",
    "myRange.groupby(\"number\").sum(\"number\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.1.지연 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식\n",
    "- 액션을 수행하기 전까지 실행 계획을 생성\n",
    "- 트랜스포메이션을 간결한 물리적 실행 계획으로 컴파일\n",
    "    - 이과정을 거치며, 전체 데이터 흐름을 최적화\n",
    "- 조건절 푸시다운(predicate pushdown)\n",
    "    - 복잡한 원시 데이터에서 특정 로우만 가져오는 필터를 가지고 있다면, 특정 레코드 하나만 읽으면 됨, 이를 데이터 소스에 위임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.environ[\"AWS_ACCESS_KEY_ID\"])\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.environ[\"AWS_SECRET_ACCESS_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan parquet [sale_notax_profit#1612L,sale_notax_amt#1613L,point_apply_amt#1614,ord_state_date#1615,qty_61#1616,dc_aff_apply_amt#1617,partner_dc_amt#1618,partner_recv_amt#1619L,ord_no_cnt#1620,ord_state#1621,dc_mem_apply_amt#1622,partner_fee#1623,ord_opt_no_cnt#1624,recv_amt#1625L,partner_sale_fee#1626L,sale_profit#1627L,amt_61#1628L,dc_etc_apply_amt#1629,amt_60#1630L,head_profit_rate#1631,pay_fee#1632,head_dc_amt#1633,partner_profit_rate#1634,partner_pay_fee#1635,... 27 more fields] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3a://musinsa.data/data/mart/orders_daily_summary], PartitionCount: 2, PartitionFilters: [((ym#1661 = 201505) || (ym#1661 = 201506))], PushedFilters: [], ReadSchema: struct<sale_notax_profit:bigint,sale_notax_amt:bigint,point_apply_amt:int,ord_state_date:string,q...\n"
     ]
    }
   ],
   "source": [
    "# IO를 줄이는 방법들\n",
    "# 1. PartitionFilters\n",
    "path = \"s3a://musinsa.data/data/mart/orders_daily_summary\"\n",
    "spark.read.parquet(path).where(\"ym=201505 or ym=201506\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [sale_notax_profit#1714L, sale_notax_amt#1715L, point_apply_amt#1716, ord_state_date#1717, qty_61#1718, dc_aff_apply_amt#1719, partner_dc_amt#1720, partner_recv_amt#1721L, ord_no_cnt#1722, ord_state#1723, dc_mem_apply_amt#1724, partner_fee#1725, ord_opt_no_cnt#1726, recv_amt#1727L, partner_sale_fee#1728L, sale_profit#1729L, amt_61#1730L, dc_etc_apply_amt#1731, amt_60#1732L, head_profit_rate#1733, pay_fee#1734, head_dc_amt#1735, partner_profit_rate#1736, partner_pay_fee#1737, ... 26 more fields]\n",
      "+- *(1) Filter (isnotnull(sale_notax_profit#1714L) && (sale_notax_profit#1714L = 34236772))\n",
      "   +- *(1) FileScan parquet [sale_notax_profit#1714L,sale_notax_amt#1715L,point_apply_amt#1716,ord_state_date#1717,qty_61#1718,dc_aff_apply_amt#1719,partner_dc_amt#1720,partner_recv_amt#1721L,ord_no_cnt#1722,ord_state#1723,dc_mem_apply_amt#1724,partner_fee#1725,ord_opt_no_cnt#1726,recv_amt#1727L,partner_sale_fee#1728L,sale_profit#1729L,amt_61#1730L,dc_etc_apply_amt#1731,amt_60#1732L,head_profit_rate#1733,pay_fee#1734,head_dc_amt#1735,partner_profit_rate#1736,partner_pay_fee#1737,... 26 more fields] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3a://musinsa.data/data/mart/orders_daily_summary/ym=201505], PartitionCount: 1, PartitionFilters: [], PushedFilters: [IsNotNull(sale_notax_profit), EqualTo(sale_notax_profit,34236772)], ReadSchema: struct<sale_notax_profit:bigint,sale_notax_amt:bigint,point_apply_amt:int,ord_state_date:string,q...\n"
     ]
    }
   ],
   "source": [
    "# 2. PushedFilters\n",
    "path = \"s3a://musinsa.data/data/mart/orders_daily_summary/ym=201505/\"\n",
    "spark.read.parquet(path).where(\"sale_notax_profit=34236772\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [sale_notax_profit#1814L]\n",
      "+- *(1) FileScan parquet [sale_notax_profit#1814L,dt#1863] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3a://musinsa.data/data/mart/orders_daily_summary/ym=201505], PartitionCount: 1, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<sale_notax_profit:bigint>\n"
     ]
    }
   ],
   "source": [
    "# 3. Column Projection\n",
    "# http://www.openkb.info/2021/02/spark-tuning-column-projection-for.html\n",
    "# parquet의 경우 columnar format이기 때문에, 전체 컬럼이 아닌 특정 컬럼만 가져와 io를 최소화\n",
    "path = \"s3a://musinsa.data/data/mart/orders_daily_summary/ym=201505/\"\n",
    "spark.read.parquet(path).select(\"sale_notax_profit\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. 액션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 트랜스포메이션을 사용해 실행 계획을 세울 수 있음\n",
    "- 액션 명령으로 실제 연산을 수행\n",
    "- 종류\n",
    "    - 콘솔에서 데이터를 보는 액션 : show\n",
    "    - 각 언어로 된 네이티브 객체에 데이터를 모으는 액션 : collect, take, top, count 등\n",
    "    - 출력 데이터소스에 저장하는 액션 : write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRange.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. 스파크 UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 드라이버 노드의 4040포트로 접근 가능\n",
    "    - 로컬 : http://localhost:4040\n",
    "    - EMR : 히스토리 서버 통해서 접근\n",
    "- 스파크 잡의 상태, 환경 설정, 클러스터 상태 등의 정보를 확인 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. 종합 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스파크는 스키마를 얻기 위해 데이터를 조금 읽고, 스파크 데이터 타입에 맞게 분석\n",
    "- 운영 환경에서는 데이터를 읽는 시점에 스키마를 엄격하게 지정하는 것을 권장\n",
    "    - spi에서 스키마가 엄격하게 필요한케이스의 경우 지정 가능하도록 수정필요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan csv [DEST_COUNTRY_NAME#1931,ORIGIN_COUNTRY_NAME#1932,count#1933] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/namjals/test/spark/Spark-The-Definitive-Guide/data/flight-data/csv/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "path = \"Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv\"\n",
    "flightData2015 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(path)\n",
    ")\n",
    "flightData2015.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실행 계획은 아래에서 위로 읽음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#1933 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#1933 ASC NULLS FIRST, 5)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#1931,ORIGIN_COUNTRY_NAME#1932,count#1933] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/namjals/test/spark/Spark-The-Definitive-Guide/data/flight-data/csv/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "# 셔플시 파티셔닝 디폴트값은 200\n",
    "# sort는 넓은 트랜스포메이션\n",
    "# 큰데이터는 적당히 큰 값이 좋은거 같음\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 액션\n",
    "flightData2015.sort(\"count\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](img/7.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.10.1. DataFrame과 SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataFrame을 가상 테이블로 등록하여 SQL 활용 가능\n",
    "- 스파크는 SQL 쿼리를 DataFrame 코드와 같은 실행 계획으로 컴파일 하므로 둘 사이의 성능 차이는 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame을 테이블로 등록\n",
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#1931], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#1931, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#1931], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#1931] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/namjals/test/spark/Spark-The-Definitive-Guide/data/flight-data/csv/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\"\n",
    "sqlWay = spark.sql(query)\n",
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#1931], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#1931, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#1931], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#1931] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/namjals/test/spark/Spark-The-Definitive-Guide/data/flight-data/csv/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "dataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스파크는 수백개의 함수를 제공\n",
    "- 다음은 특정 위치를 왕래하는 최대 비행 횟수를 구하기 위해 max 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음은 상위 5개 도착 국가를 찾아내는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql\n",
    "query = \"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "maxSql = spark.sql(query)\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataframe\n",
    "from pyspark.sql.functions import desc\n",
    "(\n",
    "    flightData2015\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\n",
    "    .sum(\"count\")\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\n",
    "    .sort(desc(\"destination_total\"))\n",
    "    .limit(5)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#1974L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#1931,destination_total#1974L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#1931], functions=[sum(cast(count#1933 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#1931, 5)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#1931], functions=[partial_sum(cast(count#1933 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#1931,count#1933] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/namjals/test/spark/Spark-The-Definitive-Guide/data/flight-data/csv/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "# explain\n",
    "# stage, sql 확인\n",
    "from pyspark.sql.functions import desc\n",
    "(\n",
    "    flightData2015\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\n",
    "    .sum(\"count\")\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\n",
    "    .sort(desc(\"destination_total\"))\n",
    "    .limit(5)\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가환성 ( 연산의 순서가 바뀌어도 상관없음 더하기나 곱하기)이 있어 각 파티션의 부분합(partial_sum)을 구하고\n",
    "- 셔플(groupbykey)이 발생하고\n",
    "- 합(sum)을 구하고\n",
    "- 셔플(sort)이 발생하고\n",
    "- 상위 5개 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](img/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 스파크 기능 둘러보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](img/8.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 운영용 애플리케이션 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 개발한 프로그램을 spark-submit 명령을 사용해 실행할 수 있음\n",
    "    - emr에 수행하는 방식\n",
    "    ![nn](img/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- deploy-mode : client, cluster\n",
    "- master(클러스터 매니저) : 스탠드얼론, 메소스, YARN\n",
    "- deploy-mode, master별 구조\n",
    "    - https://wooono.tistory.com/140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Dataset: 타입 안정성을 제공하는 구조적 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 타입 안정성 지원\n",
    "- 동적 타입 언어인 파이썬과 R에서 사용 할 수 없음\n",
    "- 다수의 소프트웨어 엔지니어가 잘 정의된 인터페이스로 상호작용하는 대규모 애플리케이션을 개발하는데 특히 유용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 구조적 스트리밍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스트림 처리용 고수준 API\n",
    "- 구조적 API로 개발된 배치 모드 연산을 스트리밍 방식으로 실행 가능\n",
    "    - 지연 시간을 줄이고 증분 처리 가능\n",
    "- 구조적 API로 프로토타입을 만들고 스트리밍 방식으로 손쉽게 전환 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 처리용 코드\n",
    "path = \"Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\"\n",
    "staticDataFrame = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(path)\n",
    ")\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   18287.0|[2011-10-28 09:00...| 70.67999999999999|\n",
      "|   18287.0|[2011-10-12 09:00...|1001.3199999999999|\n",
      "|   18287.0|[2011-05-22 09:00...|            765.28|\n",
      "|   18283.0|[2011-06-14 09:00...|103.71999999999998|\n",
      "|   18283.0|[2011-01-06 09:00...|108.44999999999997|\n",
      "|   18283.0|[2011-04-21 09:00...|117.67999999999994|\n",
      "|   18283.0|[2011-06-23 09:00...| 203.8100000000001|\n",
      "|   18283.0|[2011-09-05 09:00...|134.89999999999998|\n",
      "|   18283.0|[2011-07-14 09:00...|143.19000000000008|\n",
      "|   18283.0|[2011-10-27 09:00...|            114.65|\n",
      "|   18283.0|[2011-12-06 09:00...|208.00000000000009|\n",
      "|   18283.0|[2011-05-23 09:00...|             99.47|\n",
      "|   18283.0|[2011-02-28 09:00...|102.89999999999999|\n",
      "|   18283.0|[2011-01-23 09:00...|106.55000000000001|\n",
      "|   18283.0|[2011-11-23 09:00...| 313.6499999999997|\n",
      "|   18283.0|[2011-11-10 09:00...|114.30000000000003|\n",
      "|   18283.0|[2011-11-30 09:00...|223.61000000000016|\n",
      "|   18282.0|[2011-08-09 09:00...|             -1.45|\n",
      "|   18282.0|[2011-08-05 09:00...|            100.21|\n",
      "|   18282.0|[2011-12-02 09:00...|             77.84|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kinesis 데이터의 hit_time컬럼의 window(process_window or process_groupby)에 따라 sink 시 date로 파티션닝 하면 될 듯\n",
    "from pyspark.sql.functions import window, col, desc\n",
    "(\n",
    "    staticDataFrame\n",
    "    .selectExpr(\n",
    "        \"CustomerId\",\n",
    "        \"(UnitPrice * Quantity) as total_cost\",\n",
    "        \"InvoiceDate\")\n",
    "    .groupBy(\n",
    "        col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\n",
    "    .sum(\"total_cost\")\n",
    "    .sort(desc(\"CustomerId\"))\n",
    "    .show(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구조적 스트리밍 코드\n",
    "path = \"Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\"\n",
    "streamingDataFrame = (\n",
    "    spark\n",
    "    .readStream.format(\"csv\")\n",
    "    .schema(staticSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구조적 API로 개발된 연산을 그대로 사용\n",
    "purchaseByCustomerPerHour = (\n",
    "    streamingDataFrame\n",
    "    .selectExpr(\n",
    "        \"CustomerId\",\n",
    "        \"(UnitPrice * Quantity) as total_cost\",\n",
    "        \"InvoiceDate\")\n",
    "    .groupBy(\n",
    "        col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\n",
    "    .sum(\"total_cost\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x1297656d0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디버깅 용이하도록 memory에 write하여, customer_purchases의 이름으로 가상테이블 생성\n",
    "(\n",
    "    purchaseByCustomerPerHour.writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"customer_purchases\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   12415.0|[2011-03-03 09:00...|          16558.14|\n",
      "|   15769.0|[2011-03-17 09:00...|           10065.0|\n",
      "|      null|[2011-03-17 09:00...| 7876.000000000018|\n",
      "|   12435.0|[2011-03-16 09:00...|3978.9899999999993|\n",
      "|      null|[2011-03-03 09:00...| 3538.750000000001|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최초\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\"\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   17450.0|[2011-09-20 09:00...|          71601.44|\n",
      "|      null|[2011-11-14 09:00...|          55316.08|\n",
      "|      null|[2011-11-07 09:00...|          42939.17|\n",
      "|      null|[2011-03-29 09:00...| 33521.39999999998|\n",
      "|      null|[2011-12-08 09:00...|31975.590000000007|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 시간이 지난 후 결과\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\"\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x129727490>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# console 출력, 아마 터미널에서 동작할 것\n",
    "(\n",
    "    purchaseByCustomerPerHour.writeStream\n",
    "    .format(\"console\")\n",
    "    .queryName(\"customer_purchases2\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. 머신러닝과 고급 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 내장된 머신러닝 알고리즘 라이브러리 MLlib\n",
    "- 대용량 데이터를 대상으로 전처리, 멍잉, 모델 학습, 예측 가능\n",
    "- 구조적 스트리밍 데이터로 MLlib에서 학습시킨 모델에 적용하여 예측 가능\n",
    "- 분류, 회귀, 군집화, 딥러닝 API 제공\n",
    "- k-평균 알고리즘을 이용한 군집화 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|day_of_week|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|     Monday|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|     Monday|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|     Monday|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|     Monday|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|     Monday|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|     Monday|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|     Monday|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|     Monday|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|     Monday|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "preppedDataFrame = (\n",
    "    staticDataFrame\n",
    "    .na.fill(0)\n",
    "    .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\n",
    "    .coalesce(5)\n",
    ")\n",
    "staticDataFrame.show()\n",
    "preppedDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245903\n",
      "296006\n"
     ]
    }
   ],
   "source": [
    "# 학습 및 테스트 데이터 분리, 대략 절반으로 나눔\n",
    "trainDataFrame = (\n",
    "    preppedDataFrame\n",
    "    .where(\"InvoiceDate < '2011-07-01'\")\n",
    ")\n",
    "testDataFrame = (\n",
    "    preppedDataFrame\n",
    "    .where(\"InvoiceDate >= '2011-07-01'\")\n",
    ")\n",
    "print(trainDataFrame.count())\n",
    "print(testDataFrame.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 요일을 숫자로 매핑\n",
    "# 토요일은 6, 월요일은 1, 수치로 표현되어 토요일이 월요일보다 크다는 암묵적 의미, 이를 보완하기 위해 OneHotEncoder로 인코딩\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = (\n",
    "    StringIndexer()\n",
    "    .setInputCol(\"day_of_week\")\n",
    "    .setOutputCol(\"day_of_week_index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder로 인코딩하여 벡터 컬럼 생성\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = (\n",
    "    OneHotEncoder()\n",
    "    .setInputCol(\"day_of_week_index\")\n",
    "    .setOutputCol(\"day_of_week_encoded\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnitPrice, Quantity, day_of_week_encoded를 피처로 만듬\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = (\n",
    "    VectorAssembler()\n",
    "    .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\n",
    "    .setOutputCol(\"features\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 입력 데이터가 같은 프로세스를 거치도록 파이프라인 설정\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = (\n",
    "    Pipeline()\n",
    "    .setStages([indexer, encoder, vectorAssembler])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인을 통과하여 변환\n",
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining = fittedPipeline.transform(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, day_of_week: string, day_of_week_index: double, day_of_week_encoded: vector, features: vector]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 반복적으로 결과 데이터셋에 접근하기 위해 캐싱\n",
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = (\n",
    "    KMeans()\n",
    "    .setK(20)\n",
    "    .setSeed(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547591342.4568446"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedTest = fittedPipeline.transform(testDataFrame)\n",
    "kmModel.computeCost(transformedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+-----------------+-------------------+--------------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|day_of_week|day_of_week_index|day_of_week_encoded|            features|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+-----------------+-------------------+--------------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.79,...|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.25,...|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.65,...|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.25,...|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[2.55,...|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[0.85,...|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[4.95,...|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.69,...|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[4.25,...|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[4.25,...|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[4.95,...|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[4.95,...|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.95,...|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.95,...|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.25,...|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.65,...|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[0.85,...|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.25,...|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[4.25,...|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[4.25,...|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+-----------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformedTest.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. 저수준 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자바와 파이썬 객체를 다루는 데 필요한 다양한 기본 기능(저수준 API) 제공\n",
    "- 스파크의 거의 모든 기능은 RDD를 기반으로 만들어짐\n",
    "- DataFrame도 RDD를 기반으로 만들어졌고, 저수준 명령으로 컴파일됨\n",
    "- 원시데이터를 읽거나 다루는 용도로 RDD를 사용할 수 있지만 대부분 구조적 API를 사용하는 것이 좋음\n",
    "- RDD를 이용해 파티션과 같은 물리적 실행 특성을 결정할 수 있으므로 DataFrame 보다 더 세밀한 제어가 가능\n",
    "- 최신 버전의 스파크에서는 기본적으로 RDD를 사용하지 않지만, 비정형 데이터나 정제되지 않은 원시 데이터를 처리해야 한다면 RDD를 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. SparkR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. 스파크의 에코시스템과 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://spark-packages.org/\n",
    "- 현재 500여개의 패키지 존재\n",
    "![nn](img/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
